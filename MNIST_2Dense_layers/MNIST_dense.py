# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r2w5jYg_sljf7oa_h1Vl9b2ZpfwrOIFu
"""

!pip install tensorflow keras numpy mnist matplotlib

#import the packages/dependencies
import tensorflow as tf
import numpy as np
import mnist #get dataset from
import matplotlib.pyplot as plt #Graph
from keras.models import Sequential   #ANN
from keras.layers import Dense #The layers in the ANN
from keras.layers import Conv2D, MaxPool2D, Flatten, ZeroPadding2D, Activation, Dropout
from keras.utils  import to_categorical 

#load the dataset
train_images = mnist.train_images()  #training data images
train_labels = mnist.train_labels()  #training data labels
test_images = mnist.test_images()    #training data images
test_labels = mnist.test_labels()    #training data labels 
#Normalizing the images. Normalize the pixel values from [0,255] to [-0.5,0.5] to make our network easier to train
train_images = (train_images/255)  
test_images = (test_images/255)
#Flattening the images. Flatten each 28x28 image into a 784 dimensional vector to pass into the neural network
train_images = train_images.reshape((-1,784))
test_images = test_images.reshape((-1,784))
#print the shape of our images
print(train_images.shape)  #60,000 rows and 784 columns
print(test_images.shape)   #10,000 rows and 784 columns

#Build the model
#3 layers, 2 layers with 64 neurons and the relu function, 1 layer with 10 neurons and softmax function
model1 = Sequential()
model1.add(Dense(64, activation='relu',input_dim=784))
#model.add(Dropout(0.2))
#model.add(MaxPool2D(pool_size=(3,3),strides=1)) 
#model.add(Dense(64, activation='relu'))
model1.add(Dense(10,activation='softmax'))

model1.compile(
    optimizer='adam',
    loss = 'categorical_crossentropy',
    metrics=['accuracy']
)

#train the model
model1.fit(
    train_images,
    to_categorical(train_labels),                 #Ex. 2 it expects [0,0,1,0,0,0,0,0,0,0] 
    epochs=10,                        #the number of iterations over the entire dataset to train on
    batch_size=32,                   #number of samples per gradient update for training
)
model.summary()

#Evaluate the model
model1.evaluate(
    test_images,
    to_categorical(test_labels)
)

#Save the model
model1.save_weights('model1_weights.h5')

#predict on the first 5 test images
predictions = model1.predict(test_images[:6])
#print our model's prediction 
print(np.argmax(predictions, axis=1))
print(test_labels[:6])

print(model.weights)
np.save('model1.npy',model1.get_weights())