# -*- coding: utf-8 -*-
"""MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HQwhtB5EYf3sX2evW7vqnhERD7TLlMzC
"""

#Description: This program classifies the MNIST handwritten digit images as a number  0 - 9
!pip install tensorflow keras numpy mnist matplotlib

#import the packages/dependencies
import numpy as np
import mnist #get dataset from
import matplotlib.pyplot as plt #Graph
from keras.models import Sequential   #ANN
from keras.layers import Dense #The layers in the ANN
from keras.layers import Conv2D, MaxPool2D, Flatten, ZeroPadding2D, Activation
from keras.utils  import to_categorical

#load the dataset
train_images = mnist.train_images()  #training data images
train_labels = mnist.train_labels()  #training data labels
test_images = mnist.test_images()    #training data images
test_labels = mnist.test_labels()    #training data labels

#Normalizing the images. Normalize the pixel values from [0,255] to [-0.5,0.5] to make our network easier to train
train_images = (train_images/255)  
test_images = (test_images/255)
#Flattening the images. Flatten each 28x28 image into a 784 dimensional vector to pass into the neural network
#train_images = train_images.reshape((-1,784))
#test_images = test_images.reshape((-1,784))
train_images = train_images.reshape((-1,28,28,1))
test_images = test_images.reshape((-1,28,28,1))
#print the shape of our images
print(train_images.shape)  #60,000 rows and 784 columns
print(test_images.shape)   #10,000 rows and 784 columns

#Build the model
#3 layers, 2 layers with 64 neurons and the relu function, 1 layer with 10 neurons and softmax function
model = Sequential()
model.add(ZeroPadding2D(padding=((1,0),(1,0))))
#model.add(Dense(64, activation='relu',input_dim=784))
model.add(Conv2D(5,kernel_size=(5,5),strides=(2,2),input_shape=train_images.shape[1:]))
model.add(Activation('relu'))
model.add(MaxPool2D(pool_size=(3,3),strides=1)) 
#model.add(Activation('relu'))
model.add(Flatten())                #model.add(Dense(64, activation='relu'))
model.add(Dense(10,activation='softmax'))

#compile the model
#The loss function measures how well the model did on training and then tries to improve on it using the optimizer
model.compile(
    optimizer='adam',
    loss = 'categorical_crossentropy', #(class that are greater than 2)
    metrics = ['accuracy']
)

train_images.shape[1:]

to_categorical(train_labels).shape

#train the model
model.fit(
    train_images,
    to_categorical(train_labels),                 #Ex. 2 it expects [0,0,1,0,0,0,0,0,0,0] 
    epochs=20,                        #the number of iterations over the entire dataset to train on
    batch_size=32,                   #number of samples per gradient update for training
    validation_split=0.2
)
model.summary()

model.layers[1].output

#Evaluate the model
model.evaluate(
    test_images,
    to_categorical(test_labels)
)

#Save the model
model.save_weights('model.h5')

#predict on the first 5 test images
predictions = model.predict(test_images[:6])
#print our model's prediction 
print(np.argmax(predictions, axis=1))
print(test_labels[:6])

for i in range(0,6):
  first_image = test_images[i]
  first_image = np.array(first_image, dtype='float')
  pixels = first_image.reshape((28,28))
  plt.imshow(pixels, cmap='gray')
  plt.show()

print(model.weights)
np.save('model.npy',model.get_weights())

print(model.layers[1].get_weights()[0])
#print(model.layers[1].get_weights()[1])

print(model.layers[0].get_config())

print(model.layers[1].get_config())

print(model.layers[2].get_config())

print(model.layers[3].get_config())

print(model.layers[4].get_config())

print(model.layers[5].get_config())